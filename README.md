# ğŸ“© SMS Spam Classification Model

ğŸš€ An advanced machine learning model to classify SMS messages as Spam or Ham (Not Spam) using NLP and ensemble learning techniques.

## ğŸ” Overview

Spam messages are a common issue in SMS communication. This project builds an efficient and accurate SMS spam classifier using Natural Language Processing (NLP) and an ensemble of machine learning models.

The model achieves high accuracy through:

âœ… Text preprocessing (Cleaning, tokenization, stopword removal, stemming)

âœ… TF-IDF vectorization for feature extraction

âœ… SMOTE-based oversampling to handle class imbalance

âœ… Ensemble learning (Logistic Regression, Random Forest, XGBoost)

âœ… Probability-based voting for final predictions

## ğŸ¯ Model Performance

ğŸ“Œ The model's effectiveness is evaluated using precision, recall, F1-score, and confusion matrix.

ğŸ”¹ Accuracy: ğŸ“ˆ ~99% (Ensuring minimal false positives and false negatives)

ğŸ”¹ Precision (Spam Detection Accuracy): High precision reduces false spam alerts

ğŸ”¹ Recall (Spam Detection Sensitivity): Ensures actual spam is not missed

ğŸ”¹ F1-Score: Balances precision and recall for optimal performance
![Spam Classifier](running_vscode_.png)


## ğŸ“‚ Dataset
The model is trained on the SMSSpamCollection dataset, which contains labeled SMS messages:
(Kaggle Dataset)
âœ” Ham (Not Spam) â†’ Legitimate messages

âŒ Spam â†’ Unwanted promotional/scam messages

âœ… Data Balancing: Since spam messages are fewer in number, we use SMOTE (Synthetic Minority Oversampling Technique) to balance the dataset and improve classification performance.


## ğŸ› ï¸ Technologies Used

ğŸ”¹ Python â€“ Core programming language

ğŸ”¹ pandas & NumPy â€“ Data processing

ğŸ”¹ nltk â€“ NLP text preprocessing

ğŸ”¹ scikit-learn â€“ Machine learning algorithms

ğŸ”¹ imbalanced-learn (SMOTE)  â€“ Class balancing

ğŸ”¹ XGBoost â€“ Boosted decision trees

## ğŸš€ How to Run
1ï¸âƒ£ Install Dependencies
Before running the model, install required libraries:

```sh
pip install pandas nltk scikit-learn imbalanced-learn xgboost numpy
```

2ï¸âƒ£ Download Stopwords (if not installed)

```sh
import nltk
nltk.download('stopwords')
```

3ï¸âƒ£ Run the Script

```sh
python spam_classifier.py
```

##  ğŸ“Š Model Training & Evaluation

### ğŸ“Œ Preprocessing Steps:

Removes special characters

Converts text to lowercase

Tokenizes words

Removes stopwords

Applies stemming using Porter Stemmer

### ğŸ“Œ Feature Engineering:

Converts text into TF-IDF features

Uses bigrams to capture word relationships

### ğŸ“Œ Machine Learning Models Used:
âœ” Logistic Regression â€“ Fast & efficient for binary classification
âœ” Random Forest â€“ Handles non-linearity and prevents overfitting
âœ” XGBoost â€“ Boosted trees for better decision-making

### ğŸ“Œ Prediction Strategy:

Each model predicts the probability of a message being spam

Weighted averaging (0.3 LogReg, 0.3 RF, 0.4 XGBoost) is used for final classification

Lower spam detection threshold (0.4) ensures fewer false negatives

### ğŸ“Œ Evaluation Metrics:

âœ” Confusion Matrix

âœ” Precision, Recall, F1-score

âœ” ROC-AUC Score

## ğŸ“ˆ Results & Insights

| Metric         | Value   |
|---------------|--------|
| **Accuracy**  | 98%    |
| **Precision** | 97%    |
| **Recall**    | 96%    |
| **F1-Score**  | 96.5%  |

### ğŸ“Œ Key Insights:
âœ” High precision ensures fewer false spam alerts.
âœ” High recall captures almost all actual spam messages.
âœ” Balanced F1-score shows the model is effective for real-world use.


